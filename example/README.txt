Программа является интерпретатором для простого пользовательского языка программирования. Она принимает код на этом языке, анализирует его структуру, проверяет корректность, а затем выполняет команды. Основные этапы работы:

Лексический анализ (Lexer): разделение исходного кода на токены.
Синтаксический анализ (Parser): построение абстрактного синтаксического дерева (AST) на основе токенов.
Семантический анализ (Semantic Analyzer): проверка корректности типов данных и
Интерпретация (Interpreter): выполнение инструкций, представленных в AST.
Назначение каждого файла

lexer.py:
Этот модуль отвечает за лексический анализ.
Исходный код считывается построчно, символы и строки преобразуются в токены — минимальные смысловые единицы, такие как ключевые слова, операторы,
Пример: строка A as 5; преобразуется в токены IDENTIFIER(A), OPERATOR(as), NUMBER(5), DELIMITER(;).

parser.py:
Отвечает за синтаксический анализ.
Принимает на вход токены и строит из них абстрактное синтаксическое дерево (AST), которое представляет логическую структуру программы.
Например, выражение A as 5; преобразуется в узел AST:
Assignment:
  Identifier: A
  Value: Number(5)

semantic_analyzer.py:
Проверяет, объявлены ли переменные до их использования, корректны ли операции для указанных типов данных, совместимы ли типы данных при присваивании.
Например, для A as "text"; если A объявлена как int, будет выдана ошибка «Несовместимые типы».
interpreter.py:

Выполняет программу, основываясь на AST.
Обрабатывает операторы (например, присваивание, условные конструкции, циклы) и вычисляет значения выражений.
Хранит значения переменных и выводит результаты операций, такие как write(X).

main.py:
Основной файл, который связывает все модули.
Загружает исходный код, выполняет этапы анализа и запускает интерпретатор.
Выводит результат работы программы.
Как считывается и обрабатывается код?
Считывание кода:

Код на модельном языке подается как строка в main.py.
Например:
открытый текст
Копировать код
program var
    A int;
    B float;
begin
    A as 5;
    B as 10.5;
end.
Лексический анализ (разбиение на токены):

Код передается в LexicalAnalyzer.
Анализатор разделяет текст на токены, используя правила для:
Ключевых слов (program, var, begin, end. и т. д.).
Идентификаторов (имена переменных).
Операторов (as, mult, div и т. д.).
Чисел (целые и вещественные).
Разделителей (;, [, ] и т. д.).
Пример: строка A as 5; разбивается на токены:
открытый текст
Копировать код
Token(type=IDENTIFIER, value="A")
Token(type=OPERATOR, value="as")
Token(type=NUMBER, value="5")
Token(type=DELIMITER, value=";")
Синтаксический анализ:

Токены передаются в SyntaxAnalyzer, который строит AST.
Пример AST для программы:

Program:
  VariableDeclarations:
    - Identifier: A, Type: int
    - Identifier: B, Type: float
  StatementBlock:
    - Assignment: A = 5
    - Assignment: B = 10.5
Семантический анализ:

Проверяет:
Объявлены ли переменные.
Совместимы ли типы данных в выражениях.
Соответствуют ли условия и циклы ожидаемым типам.
Например, ошибка возникает, если пытаются присвоить строку числовой переменной.
Интерпретация:

Интерпретатор выполняет команды из AST:
Присваивает значения переменным.
Выполняет арифметические операции.
Выводит данные при вызове write.
Почему мы разбиваем код на токены?
Разбиение на токены — это ключевой этап, который делает анализ кода более простым и структурированным.

Зачем:
Абстракция:

Вместо обработки строк программы посимвольно токены представляют собой готовые блоки (например, ключевое слово, число, оператор).
Это упрощает дальнейший синтаксический и семантический анализ.
Универсальность:

Токены могут быть использованы для построения любой структуры — от дерева до списка команд.
Легкость анализа:

Каждому токену сопоставляется тип (например, IDENTIFIER), что упрощает проверку синтаксиса.
Как работает:
Анализатор ищет шаблоны (например, числа, ключевые слова, идентификаторы) и создаёт объекты Token.
Пример:
Код A as 5; будет преобразован в:
[
  Token(TokenType.IDENTIFIER, "A"),
  Token(TokenType.OPERATOR, "as"),
  Token(TokenType.NUMBER, "5"),
  Token(TokenType.DELIMITER, ";")
]
Этот процесс позволяет создать чёткую архитектуру интерпретатора, в которой каждый этап фокусируется на своей задаче: от разбиения текста на токены до выполнения инструкций.